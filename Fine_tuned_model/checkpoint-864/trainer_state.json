{
  "best_global_step": 864,
  "best_metric": 0.822320108199574,
  "best_model_checkpoint": "./version_2/checkpoint-864",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 864,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.034782608695652174,
      "grad_norm": 17.397119522094727,
      "learning_rate": 2.9812500000000003e-05,
      "loss": 12.1357,
      "step": 10
    },
    {
      "epoch": 0.06956521739130435,
      "grad_norm": 21.568485260009766,
      "learning_rate": 2.960416666666667e-05,
      "loss": 5.8953,
      "step": 20
    },
    {
      "epoch": 0.10434782608695652,
      "grad_norm": 10.110234260559082,
      "learning_rate": 2.9395833333333334e-05,
      "loss": 4.3821,
      "step": 30
    },
    {
      "epoch": 0.1391304347826087,
      "grad_norm": 8.643397331237793,
      "learning_rate": 2.91875e-05,
      "loss": 3.9836,
      "step": 40
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 8.413457870483398,
      "learning_rate": 2.897916666666667e-05,
      "loss": 3.8938,
      "step": 50
    },
    {
      "epoch": 0.20869565217391303,
      "grad_norm": 7.910055160522461,
      "learning_rate": 2.8770833333333336e-05,
      "loss": 3.5438,
      "step": 60
    },
    {
      "epoch": 0.24347826086956523,
      "grad_norm": 7.196263790130615,
      "learning_rate": 2.85625e-05,
      "loss": 3.2402,
      "step": 70
    },
    {
      "epoch": 0.2782608695652174,
      "grad_norm": 17.599119186401367,
      "learning_rate": 2.8354166666666667e-05,
      "loss": 3.0041,
      "step": 80
    },
    {
      "epoch": 0.3130434782608696,
      "grad_norm": 5.987663745880127,
      "learning_rate": 2.8145833333333334e-05,
      "loss": 2.7641,
      "step": 90
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 5.071016788482666,
      "learning_rate": 2.79375e-05,
      "loss": 2.5408,
      "step": 100
    },
    {
      "epoch": 0.3826086956521739,
      "grad_norm": 3.6033775806427,
      "learning_rate": 2.772916666666667e-05,
      "loss": 2.4154,
      "step": 110
    },
    {
      "epoch": 0.41739130434782606,
      "grad_norm": 3.223478317260742,
      "learning_rate": 2.7520833333333333e-05,
      "loss": 2.2832,
      "step": 120
    },
    {
      "epoch": 0.45217391304347826,
      "grad_norm": 3.4353253841400146,
      "learning_rate": 2.73125e-05,
      "loss": 2.1924,
      "step": 130
    },
    {
      "epoch": 0.48695652173913045,
      "grad_norm": 3.4525649547576904,
      "learning_rate": 2.7104166666666667e-05,
      "loss": 2.1576,
      "step": 140
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 2.756282329559326,
      "learning_rate": 2.6895833333333334e-05,
      "loss": 2.0928,
      "step": 150
    },
    {
      "epoch": 0.5565217391304348,
      "grad_norm": 3.1239662170410156,
      "learning_rate": 2.6687499999999998e-05,
      "loss": 2.0351,
      "step": 160
    },
    {
      "epoch": 0.591304347826087,
      "grad_norm": 2.3198606967926025,
      "learning_rate": 2.6479166666666665e-05,
      "loss": 2.003,
      "step": 170
    },
    {
      "epoch": 0.6260869565217392,
      "grad_norm": 3.2368111610412598,
      "learning_rate": 2.6270833333333333e-05,
      "loss": 1.9707,
      "step": 180
    },
    {
      "epoch": 0.6608695652173913,
      "grad_norm": 1.3498972654342651,
      "learning_rate": 2.60625e-05,
      "loss": 1.9202,
      "step": 190
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 1.5908135175704956,
      "learning_rate": 2.5854166666666667e-05,
      "loss": 1.8979,
      "step": 200
    },
    {
      "epoch": 0.7304347826086957,
      "grad_norm": 1.1150009632110596,
      "learning_rate": 2.5645833333333334e-05,
      "loss": 1.8666,
      "step": 210
    },
    {
      "epoch": 0.7652173913043478,
      "grad_norm": 1.0443670749664307,
      "learning_rate": 2.54375e-05,
      "loss": 1.8818,
      "step": 220
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.7305792570114136,
      "learning_rate": 2.522916666666667e-05,
      "loss": 1.8457,
      "step": 230
    },
    {
      "epoch": 0.8347826086956521,
      "grad_norm": 1.5320870876312256,
      "learning_rate": 2.5020833333333336e-05,
      "loss": 1.8261,
      "step": 240
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 1.1096137762069702,
      "learning_rate": 2.48125e-05,
      "loss": 1.7943,
      "step": 250
    },
    {
      "epoch": 0.9043478260869565,
      "grad_norm": 1.0613895654678345,
      "learning_rate": 2.4604166666666667e-05,
      "loss": 1.7796,
      "step": 260
    },
    {
      "epoch": 0.9391304347826087,
      "grad_norm": 1.108823299407959,
      "learning_rate": 2.4395833333333335e-05,
      "loss": 1.7537,
      "step": 270
    },
    {
      "epoch": 0.9739130434782609,
      "grad_norm": 2.3709723949432373,
      "learning_rate": 2.4187500000000002e-05,
      "loss": 1.7765,
      "step": 280
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.727941928206832,
      "eval_loss": 1.7535288333892822,
      "eval_runtime": 68.6748,
      "eval_samples_per_second": 16.76,
      "eval_steps_per_second": 8.387,
      "step": 288
    },
    {
      "epoch": 1.0069565217391305,
      "grad_norm": 1.0821582078933716,
      "learning_rate": 2.397916666666667e-05,
      "loss": 1.6535,
      "step": 290
    },
    {
      "epoch": 1.0417391304347827,
      "grad_norm": 0.8042017221450806,
      "learning_rate": 2.3770833333333333e-05,
      "loss": 1.7312,
      "step": 300
    },
    {
      "epoch": 1.0765217391304347,
      "grad_norm": 0.8120289444923401,
      "learning_rate": 2.35625e-05,
      "loss": 1.7207,
      "step": 310
    },
    {
      "epoch": 1.111304347826087,
      "grad_norm": 0.8327523469924927,
      "learning_rate": 2.3354166666666667e-05,
      "loss": 1.7046,
      "step": 320
    },
    {
      "epoch": 1.146086956521739,
      "grad_norm": 0.8042587041854858,
      "learning_rate": 2.3145833333333335e-05,
      "loss": 1.692,
      "step": 330
    },
    {
      "epoch": 1.1808695652173913,
      "grad_norm": 0.678799033164978,
      "learning_rate": 2.29375e-05,
      "loss": 1.7019,
      "step": 340
    },
    {
      "epoch": 1.2156521739130435,
      "grad_norm": 0.7618395090103149,
      "learning_rate": 2.2729166666666666e-05,
      "loss": 1.6781,
      "step": 350
    },
    {
      "epoch": 1.2504347826086957,
      "grad_norm": 0.9918834567070007,
      "learning_rate": 2.2520833333333333e-05,
      "loss": 1.6819,
      "step": 360
    },
    {
      "epoch": 1.2852173913043479,
      "grad_norm": 0.7806727290153503,
      "learning_rate": 2.23125e-05,
      "loss": 1.6688,
      "step": 370
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.1093509197235107,
      "learning_rate": 2.2104166666666667e-05,
      "loss": 1.712,
      "step": 380
    },
    {
      "epoch": 1.3547826086956523,
      "grad_norm": 0.5836107730865479,
      "learning_rate": 2.189583333333333e-05,
      "loss": 1.6613,
      "step": 390
    },
    {
      "epoch": 1.3895652173913042,
      "grad_norm": 1.0000085830688477,
      "learning_rate": 2.16875e-05,
      "loss": 1.661,
      "step": 400
    },
    {
      "epoch": 1.4243478260869566,
      "grad_norm": 0.9708404541015625,
      "learning_rate": 2.1479166666666666e-05,
      "loss": 1.6567,
      "step": 410
    },
    {
      "epoch": 1.4591304347826086,
      "grad_norm": 0.9454196691513062,
      "learning_rate": 2.1270833333333336e-05,
      "loss": 1.6619,
      "step": 420
    },
    {
      "epoch": 1.4939130434782608,
      "grad_norm": 0.8098240494728088,
      "learning_rate": 2.10625e-05,
      "loss": 1.6448,
      "step": 430
    },
    {
      "epoch": 1.528695652173913,
      "grad_norm": 0.6504934430122375,
      "learning_rate": 2.0854166666666668e-05,
      "loss": 1.6452,
      "step": 440
    },
    {
      "epoch": 1.5634782608695652,
      "grad_norm": 31.088275909423828,
      "learning_rate": 2.0645833333333335e-05,
      "loss": 1.6448,
      "step": 450
    },
    {
      "epoch": 1.5982608695652174,
      "grad_norm": 1.7890712022781372,
      "learning_rate": 2.0437500000000002e-05,
      "loss": 1.6417,
      "step": 460
    },
    {
      "epoch": 1.6330434782608696,
      "grad_norm": 1.153709053993225,
      "learning_rate": 2.022916666666667e-05,
      "loss": 1.639,
      "step": 470
    },
    {
      "epoch": 1.6678260869565218,
      "grad_norm": 1.0222724676132202,
      "learning_rate": 2.0020833333333333e-05,
      "loss": 1.6343,
      "step": 480
    },
    {
      "epoch": 1.7026086956521738,
      "grad_norm": 0.6733580231666565,
      "learning_rate": 1.98125e-05,
      "loss": 1.6174,
      "step": 490
    },
    {
      "epoch": 1.7373913043478262,
      "grad_norm": 1.8047277927398682,
      "learning_rate": 1.9604166666666668e-05,
      "loss": 1.6259,
      "step": 500
    },
    {
      "epoch": 1.7721739130434782,
      "grad_norm": 0.7083869576454163,
      "learning_rate": 1.9395833333333335e-05,
      "loss": 1.6147,
      "step": 510
    },
    {
      "epoch": 1.8069565217391306,
      "grad_norm": 0.7542096376419067,
      "learning_rate": 1.91875e-05,
      "loss": 1.6095,
      "step": 520
    },
    {
      "epoch": 1.8417391304347825,
      "grad_norm": 0.7592920064926147,
      "learning_rate": 1.8979166666666666e-05,
      "loss": 1.6176,
      "step": 530
    },
    {
      "epoch": 1.8765217391304347,
      "grad_norm": 0.6461852788925171,
      "learning_rate": 1.8770833333333333e-05,
      "loss": 1.6024,
      "step": 540
    },
    {
      "epoch": 1.911304347826087,
      "grad_norm": 0.7780216932296753,
      "learning_rate": 1.85625e-05,
      "loss": 1.594,
      "step": 550
    },
    {
      "epoch": 1.9460869565217391,
      "grad_norm": 0.6985207796096802,
      "learning_rate": 1.8354166666666668e-05,
      "loss": 1.5993,
      "step": 560
    },
    {
      "epoch": 1.9808695652173913,
      "grad_norm": 0.7877228856086731,
      "learning_rate": 1.814583333333333e-05,
      "loss": 1.6058,
      "step": 570
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.7610996837248968,
      "eval_loss": 1.5792911052703857,
      "eval_runtime": 56.6333,
      "eval_samples_per_second": 20.324,
      "eval_steps_per_second": 10.171,
      "step": 576
    },
    {
      "epoch": 2.013913043478261,
      "grad_norm": 0.6602001190185547,
      "learning_rate": 1.79375e-05,
      "loss": 1.5133,
      "step": 580
    },
    {
      "epoch": 2.048695652173913,
      "grad_norm": 0.9951708912849426,
      "learning_rate": 1.7729166666666666e-05,
      "loss": 1.5827,
      "step": 590
    },
    {
      "epoch": 2.0834782608695654,
      "grad_norm": 0.8000741004943848,
      "learning_rate": 1.7520833333333333e-05,
      "loss": 1.5888,
      "step": 600
    },
    {
      "epoch": 2.1182608695652174,
      "grad_norm": 0.7765610814094543,
      "learning_rate": 1.7312499999999997e-05,
      "loss": 1.5731,
      "step": 610
    },
    {
      "epoch": 2.1530434782608694,
      "grad_norm": 1.11387300491333,
      "learning_rate": 1.7104166666666668e-05,
      "loss": 1.5898,
      "step": 620
    },
    {
      "epoch": 2.187826086956522,
      "grad_norm": 1.0417771339416504,
      "learning_rate": 1.6895833333333335e-05,
      "loss": 1.5795,
      "step": 630
    },
    {
      "epoch": 2.222608695652174,
      "grad_norm": 0.777374804019928,
      "learning_rate": 1.6687500000000002e-05,
      "loss": 1.5733,
      "step": 640
    },
    {
      "epoch": 2.257391304347826,
      "grad_norm": 0.7387911677360535,
      "learning_rate": 1.647916666666667e-05,
      "loss": 1.5725,
      "step": 650
    },
    {
      "epoch": 2.292173913043478,
      "grad_norm": 0.9509759545326233,
      "learning_rate": 1.6270833333333334e-05,
      "loss": 1.5802,
      "step": 660
    },
    {
      "epoch": 2.3269565217391306,
      "grad_norm": 10.963128089904785,
      "learning_rate": 1.60625e-05,
      "loss": 1.5919,
      "step": 670
    },
    {
      "epoch": 2.3617391304347826,
      "grad_norm": 0.6235551834106445,
      "learning_rate": 1.5854166666666668e-05,
      "loss": 1.5799,
      "step": 680
    },
    {
      "epoch": 2.396521739130435,
      "grad_norm": 0.8615096211433411,
      "learning_rate": 1.5645833333333335e-05,
      "loss": 1.5664,
      "step": 690
    },
    {
      "epoch": 2.431304347826087,
      "grad_norm": 0.7064409852027893,
      "learning_rate": 1.54375e-05,
      "loss": 1.56,
      "step": 700
    },
    {
      "epoch": 2.466086956521739,
      "grad_norm": 0.726302981376648,
      "learning_rate": 1.5229166666666666e-05,
      "loss": 1.566,
      "step": 710
    },
    {
      "epoch": 2.5008695652173913,
      "grad_norm": 0.7566271424293518,
      "learning_rate": 1.5020833333333334e-05,
      "loss": 1.5607,
      "step": 720
    },
    {
      "epoch": 2.5356521739130433,
      "grad_norm": 0.9783593416213989,
      "learning_rate": 1.4812500000000001e-05,
      "loss": 1.5682,
      "step": 730
    },
    {
      "epoch": 2.5704347826086957,
      "grad_norm": 0.5668090581893921,
      "learning_rate": 1.4604166666666666e-05,
      "loss": 1.5686,
      "step": 740
    },
    {
      "epoch": 2.6052173913043477,
      "grad_norm": 0.8467435836791992,
      "learning_rate": 1.4395833333333334e-05,
      "loss": 1.5583,
      "step": 750
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.7339785695075989,
      "learning_rate": 1.41875e-05,
      "loss": 1.5595,
      "step": 760
    },
    {
      "epoch": 2.674782608695652,
      "grad_norm": 0.5296136140823364,
      "learning_rate": 1.3979166666666667e-05,
      "loss": 1.5654,
      "step": 770
    },
    {
      "epoch": 2.7095652173913045,
      "grad_norm": 0.7155202627182007,
      "learning_rate": 1.3770833333333334e-05,
      "loss": 1.5564,
      "step": 780
    },
    {
      "epoch": 2.7443478260869565,
      "grad_norm": 0.659907341003418,
      "learning_rate": 1.3562500000000001e-05,
      "loss": 1.5521,
      "step": 790
    },
    {
      "epoch": 2.7791304347826085,
      "grad_norm": 0.7248252630233765,
      "learning_rate": 1.3354166666666667e-05,
      "loss": 1.5642,
      "step": 800
    },
    {
      "epoch": 2.813913043478261,
      "grad_norm": 0.8067007064819336,
      "learning_rate": 1.3145833333333334e-05,
      "loss": 1.5535,
      "step": 810
    },
    {
      "epoch": 2.8486956521739133,
      "grad_norm": 0.8801413774490356,
      "learning_rate": 1.2937500000000001e-05,
      "loss": 1.5564,
      "step": 820
    },
    {
      "epoch": 2.8834782608695653,
      "grad_norm": 0.5787802934646606,
      "learning_rate": 1.2729166666666667e-05,
      "loss": 1.5599,
      "step": 830
    },
    {
      "epoch": 2.9182608695652172,
      "grad_norm": 0.6591511964797974,
      "learning_rate": 1.2520833333333334e-05,
      "loss": 1.548,
      "step": 840
    },
    {
      "epoch": 2.9530434782608697,
      "grad_norm": 1.021690011024475,
      "learning_rate": 1.23125e-05,
      "loss": 1.5674,
      "step": 850
    },
    {
      "epoch": 2.9878260869565216,
      "grad_norm": 0.6246547698974609,
      "learning_rate": 1.2104166666666667e-05,
      "loss": 1.55,
      "step": 860
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.822320108199574,
      "eval_loss": 1.5308367013931274,
      "eval_runtime": 56.3767,
      "eval_samples_per_second": 20.416,
      "eval_steps_per_second": 10.217,
      "step": 864
    }
  ],
  "logging_steps": 10,
  "max_steps": 1440,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.59028988608512e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
